{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fractalai.model import RandomDiscreteModel\n",
    "from fractalai.environment import ParallelEnvironment, AtariEnvironment\n",
    "from fractalai.swarm_wave import SwarmWave\n",
    "from fractalai.fractalmc import FractalMC\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's play Pacman with a Swarm Wave!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"MsPacman-ram-v0\"\n",
    "skip_frames = 0  # Number of frames to skip at the beginning\n",
    "render_every = 1  # print statistics every n iterations.\n",
    "\n",
    "dt_mean = 4 # Apply the same action n times in average\n",
    "dt_std = 2 # Repeat same action a variable number of times\n",
    "min_dt = 3 # Minimum number of consecutive steps to be taken\n",
    "\n",
    "n_samples = 750000 # Maximum number of samples allowed\n",
    "reward_limit = 35000  # Stop the sampling when this score is reached\n",
    "n_walkers = 88 # Maximum witdh of the tree containing al the trajectories, the bigger the better\n",
    "\n",
    "balance = 2  # Balance exploration vs exploitation\n",
    "save_data = True # Save the data generated\n",
    "prune_tree = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "env = ParallelEnvironment(name=name,env_class=AtariEnvironment,\n",
    "                          blocking=False, n_workers=8)  # We will play an Atari game\n",
    "model = RandomDiscreteModel(n_actions=env.n_actions) # The Agent will take discrete actions at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = SwarmWave(model=model, env=env, n_walkers=n_walkers, reward_limit=reward_limit, samples_limit=n_samples,\n",
    "                  render_every=render_every, balance=balance, save_data=save_data,\n",
    "                  dt_mean=dt_mean, dt_std=dt_std, accumulate_rewards=True, min_dt=min_dt,\n",
    "                  prune_tree=prune_tree, can_win=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: MsPacman-ram-v0 | Walkers: 88 | Deaths: 88 | data_size 93\n",
      "Total samples: 729874 Progress: 97.32%\n",
      "Reward: mean 33171.00 | Dispersion: 0.00 | max 33171.00 | min 33171.00 | std 0.00\n",
      "Episode length: mean 8816.43 | Dispersion 8.00 | max 8822.00 | min 8814.00 | std 1.80\n",
      "Dt: mean 3.98 | Dispersion: 5.00 | max 8.00 | min 3.00 | std 1.34\n",
      "Status: All the walkers died.\n",
      "Efficiency 0.31%\n",
      "Generated 2236 Examples | 326.42 samples per example.\n",
      "\n",
      "CPU times: user 19.5 s, sys: 788 ms, total: 20.3 s\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sw.run_swarm(print_swarm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw.render_game(sleep=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics about the data generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The game was sampled after 729874 samples, at 95.83 fps, \n",
      "4.79 times faster than human gameplay. \n",
      "It achieved an score 2.11 times better than a human playing for 2 hours.\n",
      "It took 82.79 samples per action (per game on average).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Copy paste from cell above\n",
    "total_saples = 729874\n",
    "mean_samples = 8816 # Mean episode length\n",
    "score = 33171\n",
    "time = 60 + 32\n",
    "\n",
    "score_human = 15693\n",
    "fps_atari = 20\n",
    "\n",
    "mean_fps_fai = (mean_samples / time) \n",
    "mean_fps_rel = mean_fps_fai/ fps_atari\n",
    "\n",
    "samples_action = total_saples / mean_samples\n",
    "score_ratio = score / score_human \n",
    "\n",
    "print(\"The game was sampled after {} samples, at {:.2f} fps, \\n\"\n",
    "      \"{:.2f} times faster than human gameplay. \\n\"\n",
    "      \"It achieved an score {:.2f} times better than a human playing for 2 hours.\\n\"\n",
    "      \"It took {:.2f} samples per action (per game on average).\\n\"\n",
    "      .format(total_saples, mean_fps_fai, mean_fps_rel, score_ratio, samples_action))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark against other planning algorithms: UCT & p-IW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Planning with Pixels in (Almost) Real Time](https://arxiv.org/pdf/1801.03354.pdf)\n",
    "- [Blind Search for Atari-Like Online Planning Revisited](https://www.ijcai.org/Proceedings/16/Papers/460.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Efficientcy vs.p-IW at 0.5fps in a cluster:\n",
      "It a chieved and score 2.19 times better than p-IW.\n",
      "SW is 191.65 times faster than p-IW on a cluster of M4.16xlarge running at 0.5fps.\n",
      "SW is 2518.86 times more efficient than p-IW.\n",
      "\n",
      "Efficientcy vs. p-IW 1/32fps in a cluster:\n",
      "It a chieved and score 1.44 times better than p-IW.\n",
      "SW is 3066.43 times faster than p-IW on a cluster of M4.16xlarge running at 1/32 fps\n",
      "SW is 40301.71 times more efficient than p-IW.\n",
      "\n",
      "Efficiency vs. 150k p-IW:\n",
      "It a chieved and score 1.08 times better than p-IW.\n",
      "SW is 1811.82 times more efficient than p-IW.\n",
      "\n",
      "Efficiency vs. 150k UCT:\n",
      "It a chieved and score 1.49 times better than UCT.\n",
      "SW is 1811.82 times more efficient than UCT.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N_cluster = 2\n",
    "cluster_coef =  (64 * 2.3) / (8 * 2.8) # 64 cpus @2.3GHz M4.16xlarge vs. 8cpus @2.8GHz my laptop\n",
    "plan_samples_action = 150000\n",
    "ipw_score = 30785\n",
    "\n",
    "samples_ratio = plan_samples_action / samples_action\n",
    "ipw_score_ratio = score / ipw_score\n",
    "\n",
    "ipw_fast_score = 15115.0\n",
    "ipw_fast_fps = 0.5\n",
    "ipw_fast_faster =  mean_fps_fai / ipw_fast_fps\n",
    "ipw_fast_samples_ratio = mean_fps_fai / ipw_fast_fps * cluster_coef * N_cluster\n",
    "ipw_fast_score_ratio = score / ipw_fast_score \n",
    "\n",
    "big_ipw_score = 23033.0\n",
    "big_ipw_fps = 1 / 32\n",
    "big_ipw_faster = mean_fps_fai / big_ipw_fps\n",
    "big_ipw_samples_ratio = mean_fps_fai / big_ipw_fps * cluster_coef * N_cluster\n",
    "big_ipw_score_ratio = score / big_ipw_score \n",
    "\n",
    "uct_score = 22336\n",
    "uct_score_ratio = score / uct_score \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Efficientcy vs.p-IW at 0.5fps in a cluster:\\n\"\n",
    "      \"It a chieved and score {:.2f} times better than p-IW.\\n\"\n",
    "      \"SW is {:.2f} times faster than p-IW on a cluster of M4.16xlarge running at 0.5fps.\\n\"\n",
    "      \"SW is {:.2f} times more efficient than p-IW.\\n\"\n",
    "      .format(ipw_fast_score_ratio, ipw_fast_faster, ipw_fast_samples_ratio))\n",
    "\n",
    "print(\"Efficientcy vs. p-IW 1/32fps in a cluster:\\n\"\n",
    "      \"It a chieved and score {:.2f} times better than p-IW.\\n\"\n",
    "      \"SW is {:.2f} times faster than p-IW on a cluster of M4.16xlarge running at 1/32 fps\\n\"\n",
    "      \"SW is {:.2f} times more efficient than p-IW.\\n\"\n",
    "      .format(big_ipw_score_ratio, big_ipw_faster, big_ipw_samples_ratio))\n",
    "\n",
    "print(\"Efficiency vs. 150k p-IW:\\n\"\n",
    "      \"It a chieved and score {:.2f} times better than p-IW.\\n\"\n",
    "      \"SW is {:.2f} times more efficient than p-IW.\\n\"\n",
    "      .format(ipw_score_ratio, samples_ratio))\n",
    "\n",
    "print(\"Efficiency vs. 150k UCT:\\n\"\n",
    "      \"It a chieved and score {:.2f} times better than UCT.\\n\"\n",
    "      \"SW is {:.2f} times more efficient than UCT.\\n\"\n",
    "      .format(uct_score_ratio, samples_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try FMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"MsPacman-ram-v0\"\n",
    "skip_frames = 0  # Number of frames to skip at the beginning\n",
    "dt_mean = 4 # Apply the same action n times in average\n",
    "dt_std = 2 # Repeat same action a variable number of times\n",
    "min_dt = 4 # Minimum number of consecutive steps to be taken\n",
    "n_samples = 2000 # Maximum number of samples allowed\n",
    "reward_limit = 100000  # Stop the sampling when this score is reached\n",
    "n_walkers = 50 # Maximum witdh of the tree containing al the trajectories\n",
    "render_every = 1  # print statistics every n iterations.\n",
    "balance = 2  # Balance exploration vs exploitation\n",
    "time_horizon = 30 # How deep we want each cone to be on average\n",
    "min_horizon = 30 # Minimum time horizon for the cone\n",
    "clone_seeds = False # Clone random seeds when getting ale state. If false env is stochastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fmc_env = ParallelEnvironment(name=name, env_class=AtariEnvironment, clone_seeds=clone_seeds,\n",
    "                          blocking=False, n_workers=8)  # We will play an Atari game\n",
    "fmc_model = RandomDiscreteModel(n_actions=fmc_env.n_actions) # The Agent will take discrete actions at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmc = FractalMC(model=fmc_model, env=fmc_env, max_walkers=n_walkers,\n",
    "                reward_limit=reward_limit, max_samples=n_samples,\n",
    "                render_every=render_every, balance=balance, time_horizon=time_horizon,\n",
    "                min_horizon=min_horizon, dt_mean=dt_mean, dt_std=dt_std, accumulate_rewards=True,\n",
    "                min_dt=min_dt, can_win=False, update_parameters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fmc.run_agent(print_swarm=False, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-bert as a proof of concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Create a super-human gym-like Env\n",
    "### - Sample games uring [ray](https://ray.readthedocs.io/en/latest/)\n",
    "### - True action inside info\n",
    "### - Hack OpenAI [baselines](https://github.com/openai/baselines)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters for the swarm used in the hacked Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swarm_kwargs = dict(dt_mean=10,  # Apply the same action n times in average\n",
    "                    dt_std=5,  # Repeat same action a variable number of times\n",
    "                    min_dt=5,  # Minimum number of consecutive steps to be taken\n",
    "                    samples_limit=300000,  # 200000  # Maximum number of samples allowed\n",
    "                    reward_limit=5000,  # Stop the sampling when this score is reached\n",
    "                    n_walkers=50,  # Maximum width of the tree containing al the trajectories\n",
    "                    render_every=100,  # print statistics every n iterations.\n",
    "                    balance=2,  # Balance exploration vs exploitation\n",
    "                    save_data=True,  # Save the data generated\n",
    "                    accumulate_rewards=True,\n",
    "                    prune_tree=True)\n",
    "\n",
    "generator_kwargs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same interface as OpenAI baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires OpenAI baslines\n",
    "from fractalai.datasets.baselines import make_atari_env\n",
    "\n",
    "env_id = \"QbertNoFrameskip-v4\"\n",
    "num_env = 2\n",
    "seed = 10\n",
    "n_actors = 8\n",
    "hacked_env = make_atari_env(env_id, num_env, seed, wrapper_kwargs=None, start_index=0, swarm_kwargs=swarm_kwargs,\n",
    "                           generator_kwargs=generator_kwargs, n_actors=n_actors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill up the buffer of played games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "obs = hacked_env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see how it plays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "ends = [False, False]\n",
    "while not ends[0]:\n",
    "    observation, rewards, ends, infos = hacked_env.step(\"IGNORE THIS\")\n",
    "    \n",
    "    hacked_env.render()\n",
    "    time.sleep(0.025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fractalai.datasets.mlswarm import MLWave, MLFMC\n",
    "from fractalai.datasets.data_generator import DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"MsPacman-v0\"\n",
    "skip_frames = 0  # Number of frames to skip at the beginning\n",
    "dt_mean = 2  # Apply the same action n times in average\n",
    "dt_std = 2.5 # Repeat same action a variable number of times\n",
    "min_dt = 1 # Minimum number of consecutive steps to be taken\n",
    "n_samples = 100000#200000  # Maximum number of samples allowed\n",
    "reward_limit = 10000  # Stop the sampling when this score is reached\n",
    "n_walkers = 75  # Maximum witdh of the tree containing al the trajectories\n",
    "render_every = 1  # print statistics every n iterations.\n",
    "balance = 2  # Balance exploration vs exploitation\n",
    "save_data = True # Save the data generated\n",
    "\n",
    "prune_tree = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = MLWave(model=model, env=env, n_walkers=n_walkers, reward_limit=reward_limit, samples_limit=n_samples,\n",
    "                  render_every=render_every, balance=balance, save_data=save_data,\n",
    "                  dt_mean=dt_mean, dt_std=dt_std, accumulate_rewards=True, min_dt=min_dt,\n",
    "                  prune_tree=prune_tree)\n",
    "datagen = DataGenerator(sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save whole games into a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_games = 1\n",
    "output_folder = \"dataset\"\n",
    "for i in range(n_games):\n",
    "    datagen.save_run(output_folder, render=True, print_swarm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate examples for supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "example_gen = datagen.example_generator()\n",
    "obs, action, reward, next_obs, end = next(example_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate batches of examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "batch_gen = datagen.batch_generator()\n",
    "obs, action, reward, next_obs, end = next(batch_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
